\label[chapter_4]
\chap Inverse Reinforcement Learning
In this section, we would like to introduce Inverse Reinforcement Learning (IRL) as an extension of Reinforcement Learning (RL) technique. Motivation for this technique was~\cite[Abbeel2010-11-05] presenting autonomous helicopter capable of performing aerobatics after observing experts (see figure~\ref[hel]).

\enditems
\midinsert \clabel[hel]{Autonomous Helicopter Aerobatics through IRL}
\picw=15cm \cinspic helicopter.png
\caption/f One helicopter while performing one of the
airshows. It was trained trough IRL by observing experts (from:~\cite[Abbeel2010-11-05]).
airshows.
\endinsert

\sec Reinforcement Learning and Markov Decision Process
Reinforcement Learning is very popular machine learning technique to solve situations where the agent does not know the payoff it will receive for its actions, so at first it has to explore the environment by taking random steps to learn what is an actual payoff for any action in given situation. To some extent, this can be seen as a very simple form of trial and error learning similar to learning experienced by humans during their lives.

In RL agent’s goal is to choose right action in any states he is currently in to maximize his feature discounted reward. That is, to find optimal {\bf policy}. To give a formal definition of problem we need to formalize (finite) Markov decision process (MDP) first. An MDP notation in~\cite[Abbeel2004] is given as a tuple $\left (S,A,P_{sa},\gamma ,R \right )$, where

\begitems 
* S is a finite set of {\bf states} with cardinality N
* $A=\left \{ a_{1},\cdots,a_{k} \right \}$ represents set of k {\bf actions}
* $P_{sa} \left ( . \right )$ are the state {\bf transitions probabilities} upon taking action a in state s
* $\gamma~\in~\left ( 0,1 \right )$ is the {\bf discount factor}
*$R:S \rightarrow R$ is the {\bf reinforcment (reward) function} is bounded in absolute value~by~R_{max}
\enditems

Further on we will write notation R(s,~a) as R(s) for simplicity in equations. A policy is defined as any mapping $\pi:~S~\rightarrow~A$, and the {\bf value function} for a policy $\pi$ is given as the expectation over the distribution of the sequence of states we visit by executing policy $\pi$: 

\medskip
\centerline{\eqmark~${{V}^{\pi }}\left( {{s}_{1}} \right)=E\left[ R\left( {{s}_{1}} \right)+\gamma R\left( {{s}_{2}} \right)+{{\gamma }^{2}}R({{s}_{3}})+...|\pi  \right],{{s}_{1...N}}\in S$}
\medskip

{\bf Q-function} is given as:

\medskip
\centerline{\eqmark~${{Q}^{\pi }}(s,a)=R(s)+\gamma {{E}_{{s}'\sim ~{{P}_{sa(.)}}}}[{{V}^{\pi }}({s}')]$}
\medskip

where notation ${s}'\sim P_{sa(.)}$ means the expectation with respect to state  ${s}'$ distributed according to $P_{sa(.)}$. Assuming that MDP can model all agent's decision one can find optimal policy $\pi ^{*}$ such that $V^{\pi}(s)$ is maximized. Algorithms finding optimal policies are based on two basic properties of MDPs. All $s\in S,a\in A,V^{\pi}$ and $Q^{\pi}$ for an MDP satisfy (Bellman Equations):

\medskip
\label[eq1]
\centerline{\eqmark~${{V}^{\pi }}(s)=R(s)+\gamma \sum\limits_{s'}^{{}}{{{P}_{s\pi (s)}}(s'){{V}^{\pi }}({s}')}$}
\medskip

\medskip
\label[eq2]
\centerline{\eqmark~${{Q}^{\pi }}(s,a)=R(s)+\gamma \sum\limits_{s'}^{{}}{{{P}_{sa}}(s'){{V}^{\pi }}({s}')}$}
\medskip

moreover, the policy is optimal policy for an MDP if and only if, for all $s\in S$ holds (Bellman Optimality):

\medskip
\centerline{\eqmark~$\pi (s)\in {\arg\\max}\limits_{a \in A}{{Q}^{\pi }}(s,a)$}
\medskip

Illustration of RL process to obtain optimal policy is in figure~\ref[rl]. Inputting environment model (MDP) and specifying reward function R(s) one can get optimal policy trough Reinforcement Learning.

\midinsert \clabel[rl]{The reinforcement learning framework}
\picw=15cm \cinspic rl_des.png
\caption/f The reinforcement learning framework (from:~\cite[tI5eoDw2qgj5MnYJ]).
\endinsert

\sec Inverse Reinforcement Learning
Inverse Reinforcement Learning (IRL) is a form of Apprenticeship Learning in MDP where in contrast to RL reward function is not known. Instead, few trajectories demonstrating expert performing the task to learn are available. The goal of IRL is then to use those trajectories and learn reward function that can explain expert’s behavior. Recovering reward function first and using it to generate desirable behavior (trough RL) is what makes IRL standout from other supervised learning techniques as most of them is learning policy as a mapping from states to actions right away. According to~\cite[Abbeel2004]: \uv{reward function provides a much more parsimonious description of behavior.} As is mention in~\cite[Abbeel2004]: \uv{the entire field of RL is founded on the presupposition that reward function, rather than the policy, is the most succinct, robust, and transferable definition of the task.} IRL is especially useful when we are attempting to learn intelligent agent that can behave successfully in our domain where is hard to define reward function; it is extremely labor or designer has only limited knowledge of it. RTS games are excellent examples of that as results of actions are not immediately observable. In figure~\ref[irl] one can see IRL framework in contrast to RL framework illustrated in figure~\ref[rl] we are using optimal policy to learn reward function.

\medskip \clabel[irl]{The Inverse reinforcement learning framework}
\picw=15cm \cinspic irl_des.png
\caption/f The Inverse reinforcement learning framework (from:~\cite[tI5eoDw2qgj5MnYJ]).
\medskip

\sec Solving Inverse Reinforcement Learning problem in Finite State Space
As was shown in~\cite[Abbeel2004] a solution to IRL problem in Finite State Space (for finite MDP) can be found trough {\bf linear programming} (LP). LP can be formulated based on the characterization of the {\bf Solution Set} consisting of all reward functions for which given policy is optimal. Following theorem from~\cite[Abbeel2004] characterizes the Solution Set:

\medskip
\label[definice]
\def\definice {\numberedpar B{Definition}} 
\definice Let a finite state space S, a set of actions $A=\{a_{1},\cdots ,a_{k}\}$, transition probability matricies $\{P_{a}\}$, and a discount factor $\gamma \in (0,1)$ be given. Then the policy $\pi$ given by $\pi (s)\equiv a_{1}$ is optimal if and only if, for all $a= a_{1},\cdots ,a_{k}$, the reward R satisfies

\medskip
\label[eqReq]
\centerline{\eqmark~$({{P}_{{{a}_{1}}}}-{{P}_{a}}){{(I-\gamma {{P}_{{{a}_{1}}}})}^{-1}}R\succeq 0$}
\medskip

Proof of~\ref[definice] is based on substitution equation~\ref[eq1] to~\ref[eq2] from theorem written above and can be found in~\cite[Abbeel2004]. Before giving LP formulation, two problems with this characterization of the Solution Set need to be considered. First, any constant vector is always a solution of R, and second, many solutions satisfying equation~\ref[eqReq] exist so choosing one R may impose challenge. One of the ways described in~\cite[Abbeel2004] to overcome the problem of choosing R is to demand that selected R makes given policy $\pi$ optimal and favors solutions that make any deviation from $\pi$ costly as possible. From all R functions satisfying~\ref[eqReq] and $|R(s)|\leq R_{max}\forall s$, we pick one maximizing:

\medskip
\centerline{\eqmark~$\sum\limits_{s\in S}{Q^{\pi }}{{(s,{a}_{1})}-{\max\limits_{a\in A\backslash {a}_{1}} Q^{\pi }(s,a)}}$}
\medskip

Then we can formulate the optimization problem as LP which we want to maximize:

\medskip
  \centerline{\sum\limits_{i=1}^{N}{\min\limits_{a\in \{{a_{2}},\cdots ,{a_{k}}\}}}\{({{P}_{{{a}_{1}}}}(i)-{{P}_{a}}(i)){{(I-\gamma {{P}_{{{a}_{1}}}})}^{-1}}R\}}
\medskip
   \centerline{s.t.\ ({{P}_{{{a}_{1}}}}-{{P}_{a}}){{(I-\gamma {{P}_{{{a}_{1}}}})}^{-1}}R\ge 0,\ \forall a\in A\backslash {{a}_{1}}}
\medskip
  \centerline{\left| {{R}_{i}} \right|\le {{R}_{\max }},i=1,...,N }
\medskip

In our opinion, IRL can make a useful addition to already existing approaches (more on this in section~\ref[techniques]) how to employ huge data sets with examples of professional play in games like StarCraft and bootstrap some of the expert level domain knowledge to AI agents.Due to the complexity of our domain and computational feasibility, we currently restrict ourselves to the simplest cases of IRL. We consider only IRL in Finite State Space for our experiments (to do that we are using clustering as presented in~\ref[chapter_6]). Nevertheless, human StarCraft players seem to also reason about finite number of typical game situations (states). Evidence of that in StarCraft domain is the existence of many guides\fnote{\url{http://wiki.teamliquid.net/starcraft/Main_Page}} how to play in given state of affairs.
